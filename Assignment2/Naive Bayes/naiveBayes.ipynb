{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "import pickle\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sn\n",
    "import collections\n",
    "from tqdm import tqdm\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train=pd.read_csv(\"training.1600000.processed.noemoticon.csv\",encoding='latin-1',header=None)\n",
    "test=pd.read_csv(\"testdata.manual.2009.06.14.csv\",encoding='latin-1',header=None)\n",
    "testRef=np.array(test)\n",
    "check=testRef[testRef[:,0]!=2]\n",
    "test=pd.DataFrame(check)\n",
    "# print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/1600000 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  5%|▍         | 74315/1600000 [00:00<00:02, 743143.62it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yahan aa bhi rha hai kya\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 145863/1600000 [00:00<00:01, 734621.09it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 14%|█▎        | 217507/1600000 [00:00<00:01, 729068.17it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 18%|█▊        | 289428/1600000 [00:00<00:01, 726080.57it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 22%|██▏       | 351077/1600000 [00:00<00:01, 689319.10it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 26%|██▌       | 416506/1600000 [00:00<00:01, 678418.94it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 30%|███       | 487940/1600000 [00:00<00:01, 688807.94it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 34%|███▍      | 550964/1600000 [00:00<00:01, 633118.62it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 38%|███▊      | 615758/1600000 [00:00<00:01, 637489.94it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 42%|████▏     | 677263/1600000 [00:01<00:01, 629604.64it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 46%|████▋     | 742952/1600000 [00:01<00:01, 637547.65it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 51%|█████     | 812948/1600000 [00:01<00:01, 655069.00it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 55%|█████▌    | 884756/1600000 [00:01<00:01, 672777.71it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 59%|█████▉    | 951718/1600000 [00:01<00:00, 670743.02it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 64%|██████▎   | 1018573/1600000 [00:01<00:00, 661723.66it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 68%|██████▊   | 1090222/1600000 [00:01<00:00, 677252.19it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 72%|███████▏  | 1157949/1600000 [00:01<00:00, 669704.83it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 77%|███████▋  | 1224940/1600000 [00:01<00:00, 656842.96it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 81%|████████  | 1290886/1600000 [00:01<00:00, 657624.67it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 85%|████████▌ | 1367104/1600000 [00:02<00:00, 685848.41it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 90%|█████████ | 1441469/1600000 [00:02<00:00, 702222.60it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 95%|█████████▍| 1512032/1600000 [00:02<00:00, 695998.53it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|██████████| 1600000/1600000 [00:02<00:00, 559120.46it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/800000 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  8%|▊         | 63982/800000 [00:00<00:01, 639812.98it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 16%|█▋        | 130438/800000 [00:00<00:01, 647041.11it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 25%|██▍       | 198634/800000 [00:00<00:00, 657133.81it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 30%|███       | 242454/800000 [00:00<00:01, 435221.24it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 36%|███▌      | 284133/800000 [00:00<00:02, 228881.30it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 45%|████▍     | 359683/800000 [00:00<00:01, 289398.39it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 50%|█████     | 403239/800000 [00:01<00:01, 268646.10it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 55%|█████▌    | 440632/800000 [00:02<00:04, 78328.92it/s] \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 59%|█████▉    | 475391/800000 [00:03<00:05, 64747.69it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 65%|██████▌   | 521314/800000 [00:03<00:03, 87226.05it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 69%|██████▊   | 549319/800000 [00:04<00:05, 49978.08it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 73%|███████▎  | 582208/800000 [00:05<00:04, 45536.96it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 77%|███████▋  | 619991/800000 [00:06<00:04, 41183.31it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 82%|████████▏ | 658114/800000 [00:06<00:02, 55130.37it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 86%|████████▋ | 691809/800000 [00:06<00:01, 72439.60it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 90%|█████████ | 720807/800000 [00:06<00:00, 89739.68it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 96%|█████████▌| 766386/800000 [00:06<00:00, 115158.52it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|██████████| 800000/800000 [00:07<00:00, 111703.86it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/800000 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  5%|▍         | 38214/800000 [00:00<00:01, 382137.63it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 14%|█▎        | 109641/800000 [00:00<00:01, 444087.08it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 17%|█▋        | 139094/800000 [00:00<00:02, 227052.11it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 23%|██▎       | 182162/800000 [00:01<00:04, 130929.39it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 25%|██▌       | 200798/800000 [00:02<00:12, 48723.60it/s] \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 34%|███▍      | 273525/800000 [00:02<00:07, 67662.39it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 39%|███▉      | 315504/800000 [00:02<00:05, 86344.33it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 46%|████▋     | 371739/800000 [00:02<00:03, 109469.76it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 52%|█████▏    | 419476/800000 [00:03<00:03, 101949.26it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 61%|██████    | 486964/800000 [00:03<00:02, 108207.08it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 69%|██████▉   | 554097/800000 [00:03<00:01, 126022.86it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 78%|███████▊  | 623510/800000 [00:04<00:01, 167035.56it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 82%|████████▏ | 658408/800000 [00:04<00:00, 163229.49it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 87%|████████▋ | 697887/800000 [00:04<00:00, 114688.09it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 91%|█████████ | 725181/800000 [00:05<00:00, 89465.95it/s] \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|██████████| 800000/800000 [00:05<00:00, 135480.22it/s]\u001b[A\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Negative Word :  6745361\n",
      "---------------------\n",
      "6442058 6745361 361469 296929\n",
      "[0.5, 0.5]\n",
      "550180\n",
      "2\n",
      "Time taken for training is  41.37784218788147\n"
     ]
    }
   ],
   "source": [
    "def mainFunction(dataAttr,ngram):\n",
    "    \n",
    "    def trainInitialization(dataAttr):  #0 for Negative and 4 for Positive\n",
    "        vocabData=[]\n",
    "        dataForVocab=np.array(dataAttr[5])\n",
    "        classData=np.array(dataAttr[0])\n",
    "        \n",
    "        print(\"yahan aa bhi rha hai kya\")\n",
    "        \n",
    "        adverb=[\"RB\",\"RBR\",\"RBS\",\"WRB\"]\n",
    "        verb=[\"VB\",\"VBD\",\"VBG\",\"VBN\",\"VBP\",\"VBZ\"]\n",
    "        adjective=[\"JJ\",\"JJR\",\"JJS\"]\n",
    "        \n",
    "        tempIndex=0\n",
    "        for sentence in tqdm(dataForVocab):\n",
    "            temp=sentence.replace(\",\",\" \").replace(\".\",\" \").split()\n",
    "            \n",
    "            for words in temp:\n",
    "                vocabData.append(words)\n",
    "                \n",
    "# Below is the code for POS tagging which would be uncommented when needed.\n",
    "            \n",
    "#             newSentence=finalTaggedList[tempIndex]\n",
    "#             for (word,tagged) in newSentence:\n",
    "#                 if \"RB\" in tagged:\n",
    "#                     temp=[word]*3\n",
    "#                     vocabData=vocabData+temp\n",
    "#             tempIndex+=1\n",
    "                \n",
    "            if(ngram==\"bi\"):\n",
    "                temp2 = sentence.replace(\",\",\" \").replace(\".\",\" \").split()\n",
    "                for i in range(len(temp2)-1):\n",
    "                    vocabData.append(temp2[i]+\" \"+temp2[i+1])\n",
    "        \n",
    "#         with open('vocabData','wb') as f:\n",
    "#             #It is being stored for clean data only\n",
    "#             pickle.dump(vocabData,f)\n",
    "                \n",
    "\n",
    "        trainNegative=dataAttr[dataAttr[0]==0]\n",
    "        trainPositive=dataAttr[dataAttr[0]==4]\n",
    "\n",
    "        vocabPositiveData,vocabNegativeData=[],[]\n",
    "        trainPositiveTweet,trainNegativeTweet=trainPositive[5],trainNegative[5]\n",
    "        \n",
    "        posTweetIndex=0\n",
    "        for sentence in tqdm(trainPositiveTweet):\n",
    "            temp=sentence.replace(\",\",\" \").replace(\".\",\" \").split()\n",
    "            for words in temp:\n",
    "                vocabPositiveData.append(words)\n",
    "  \n",
    "\n",
    "#   Below is the code that would be uncommented when needed.\n",
    "#               posSentence=positiveTaggedList[posTweetIndex]\n",
    "#             for (word,tagged) in posSentence:\n",
    "#                 if \"RB\" in tagged:\n",
    "#                     temp=[word]*3\n",
    "#                     vocabPositiveData=vocabPositiveData+temp\n",
    "#             posTweetIndex+=1\n",
    "            \n",
    "            \n",
    "            if(ngram==\"bi\"):\n",
    "                temp2 = sentence.replace(\",\",\" \").replace(\".\",\" \").split()\n",
    "                for i in range(len(temp2)-1):\n",
    "                    vocabPositiveData.append(temp2[i]+\" \"+temp2[i+1])\n",
    "        \n",
    "#         with open('vocabPositiveData','wb') as f:\n",
    "#             #It is being stored for clean data only, vocabPositiveData\n",
    "#             pickle.dump(vocabPositiveData,f)\n",
    "\n",
    "        negTweetIndex=0\n",
    "        for sentence in tqdm(trainNegativeTweet):\n",
    "            temp=sentence.replace(\",\",\" \").replace(\".\",\" \").split()\n",
    "            for words in temp:\n",
    "                vocabNegativeData.append(words)\n",
    "\n",
    "# Below is the code for POS tagging which would be uncommented when needed.\n",
    "\n",
    "#             negSentence=negativeTaggedList[negTweetIndex]\n",
    "#             for (word,tagged) in negSentence:\n",
    "#                 if \"RB\" in tagged:\n",
    "#                     temp=[word]*3\n",
    "#                     vocabNegativeData=vocabNegativeData+temp\n",
    "#             negTweetIndex+=1  \n",
    "            \n",
    "                \n",
    "            if(ngram==\"bi\"):\n",
    "                temp2 = sentence.replace(\",\",\" \").replace(\".\",\" \").split()\n",
    "                for i in range(len(temp2)-1):\n",
    "                    vocabNegativeData.append(temp2[i]+\" \"+temp2[i+1])\n",
    "                    \n",
    "            \n",
    "        \n",
    "#         with open('vocabNegativeData','wb') as f:\n",
    "#             #It is being stored for clean data only, vocabPositiveData\n",
    "#             pickle.dump(vocabNegativeData,f)\n",
    "\n",
    "        return vocabData,vocabPositiveData,vocabNegativeData,classData\n",
    "    \n",
    "    start=time.time()\n",
    "    \n",
    "    (vocabData,vocabPositiveData,vocabNegativeData,classData)=trainInitialization(dataAttr)\n",
    "    \n",
    "    def afterTrainInitialization():\n",
    "        totalPositiveWord=len(vocabPositiveData)\n",
    "        totalNegativeWord=len(vocabNegativeData)\n",
    "        print(\"Total Negative Word : \",totalNegativeWord)\n",
    "        print(\"---------------------\")\n",
    "        countPositiveWord=Counter(vocabPositiveData)\n",
    "        countNegativeWord=Counter(vocabNegativeData)\n",
    "        print(totalPositiveWord,totalNegativeWord,len(countPositiveWord),len(countNegativeWord))\n",
    "        return totalPositiveWord,totalNegativeWord,countPositiveWord,countNegativeWord\n",
    "    \n",
    "    (totalPositiveWord,totalNegativeWord,countPositiveWord,countNegativeWord)= afterTrainInitialization()\n",
    "    \n",
    "    def phi():\n",
    "        temp=[]\n",
    "        classCount=Counter(classData)\n",
    "        total=sum(classCount.values())\n",
    "        temp.append((classCount[0]+1)/(total+2))\n",
    "        temp.append((classCount[4]+1)/(total+2))\n",
    "        return temp\n",
    "\n",
    "    phii=phi()\n",
    "    print(phii)\n",
    "    \n",
    "    def indexForWords():\n",
    "        vocabWords=Counter(vocabData)\n",
    "        words=list(vocabWords.keys())\n",
    "\n",
    "        wordsIndex = collections.defaultdict(lambda : -1)\n",
    "        p=0\n",
    "        for word in words:\n",
    "            wordsIndex[word]=p\n",
    "            p+=1\n",
    "\n",
    "        #Checking default dict in python\n",
    "\n",
    "        sizeVocab=(len(words))\n",
    "        print(sizeVocab)\n",
    "        return wordsIndex,words,sizeVocab\n",
    "    \n",
    "    wordsIndex,words,sizeVocab=indexForWords()\n",
    "    \n",
    "    def theta():\n",
    "        temp1,temp2=[],[]\n",
    "        for x in words:\n",
    "            temp1.append((countNegativeWord[x]+1)/(totalNegativeWord+sizeVocab))\n",
    "            temp2.append((countPositiveWord[x]+1)/(totalPositiveWord+sizeVocab))\n",
    "\n",
    "        temp3=[temp1,temp2]\n",
    "        print(len(temp3))\n",
    "        return temp3\n",
    "\n",
    "    theta=theta()\n",
    "    \n",
    "    return theta,wordsIndex,phii,totalPositiveWord,totalNegativeWord,sizeVocab\n",
    "\n",
    "start=time.time()\n",
    "(theta,wordsIndex,phii,totalPositiveWord,totalNegativeWord,sizeVocab)=mainFunction(cleanTrainData.copy(),\"uni\")\n",
    "print(\"Time taken for training is \",time.time()-start)\n",
    "\n",
    "#The second parameter of mainFunction is for feature engineering that takes \"bi\" for including bigram with unigram\n",
    "#Here we have to call to get theta for testing the generated model\n",
    "# out=mainFunction(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testData(data,ngram):\n",
    "    attrPanda=pd.DataFrame(data)\n",
    "    tweets=attrPanda[5]\n",
    "    actualValue=[]\n",
    "    prob=[]\n",
    "    k=0\n",
    "    for sentence in tqdm(tweets):\n",
    "        resPos,resNeg=0,0\n",
    "        \n",
    "        temp1=sentence.replace(\",\",\" \").replace(\".\",\" \").split()\n",
    "        if(ngram==\"bi\"):\n",
    "                temp2 = sentence.replace(\",\",\" \").replace(\".\",\" \").split()\n",
    "                for i in range(len(temp2)-1):\n",
    "                    temp1.append(temp2[i]+\" \"+temp2[i+1])\n",
    "        \n",
    "        k+=1\n",
    "        \n",
    "        for words in temp1:\n",
    "\n",
    "            if(wordsIndex[words]!=-1):\n",
    "                resPos+=math.log(theta[1][wordsIndex[words]])\n",
    "                resNeg+=math.log(theta[0][wordsIndex[words]])\n",
    "            else:\n",
    "                resPos+=math.log(1/(totalPositiveWord+sizeVocab))\n",
    "                resNeg+=math.log(1/(totalNegativeWord+sizeVocab))\n",
    "        resPos+=math.log(phii[1])\n",
    "        resNeg+=math.log(phii[0])\n",
    "        prob.append(resPos)\n",
    "        if(resPos>resNeg):\n",
    "            actualValue.append(4)\n",
    "        else:\n",
    "            actualValue.append(0)\n",
    "    originalValue=list(attrPanda[0])\n",
    "    return originalValue,actualValue,prob\n",
    "\n",
    "(original,tested,probability)=testData(test.copy(),\"uni\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing 148 29 29 153\n",
      "Accuracy is  83.84401114206128\n"
     ]
    }
   ],
   "source": [
    "%matplotlib qt\n",
    "def cMatrixAndAccuracy(original,tested):\n",
    "    l=len(original)\n",
    "    c00,c04,c40,c44=0,0,0,0\n",
    "    for (o,t) in zip(original,tested):\n",
    "        if o==0 and t==0:\n",
    "            c00+=1\n",
    "        elif o==0 and t==4:\n",
    "            c04+=1\n",
    "        elif o==4 and t==0:\n",
    "            c40+=1\n",
    "        else:\n",
    "            c44+=1\n",
    "    accuracy=((c00+c44)/l)*100\n",
    "    data=[[c00,c40],[c04,c44]]\n",
    "    print(\"printing\",c00,c40,c04,c44)\n",
    "#     matrix=pd.DataFrame(data,columns=['Class0','Class4'],index=['Class0', 'Class4'])\n",
    "    return accuracy,data\n",
    "\n",
    "accuracy,matrix=cMatrixAndAccuracy(original,tested)\n",
    "print(\"Accuracy is \",accuracy)\n",
    "     \n",
    "plt.figure(figsize= (7,4))\n",
    "\n",
    "sn.heatmap(matrix,cmap='OrRd', annot=True,cbar=False,fmt='d', xticklabels=[0,4], yticklabels=[0,4])\n",
    "\n",
    "# plt.title(\"Confusion Matrix\", fontsize = 10)\n",
    "plt.xlabel(\"Actual Class\", fontsize = 10)\n",
    "plt.ylabel(\"Predicted Class\", fontsize = 10)\n",
    "plt.show()\n",
    "\n",
    "# print(\"Confusion matrix is :\",\"\\n\")\n",
    "# print(matrix)\n",
    "# print(\"Time taken is : \", time.time()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Accuracy for test when unigram is used : 81.33704735376045%__\n",
    "\n",
    "__Accuracy for test when bigram is used : 83.84401114206128 %__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 1:b__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing 94 83 85 97\n",
      "printing 177 0 182 0\n",
      "printing 0 177 0 182\n",
      "Accuracy in case of random generation is :  53.2033426183844\n",
      "Accuracy in case of Majority Predicton with 0 as majority is :  49.30362116991643\n",
      "Accuracy in case of Majority Predicton with 4 as majority is :  50.69637883008357\n"
     ]
    }
   ],
   "source": [
    "def differentAccuracies():\n",
    "    testOriginal=list(test[0])\n",
    "    lengthTest=len(testOriginal)\n",
    "#     print(lengthTest)\n",
    "    testRandom=[]\n",
    "\n",
    "    for i in range(lengthTest):\n",
    "        testRandom.append(random.choice([0,4]))\n",
    "    \n",
    "    testMajority0=[0]*lengthTest\n",
    "    testMajority4=[4]*lengthTest\n",
    "\n",
    "    #Accuracy for randomly assigned outputs\n",
    "    accuRandom,matRandom=cMatrixAndAccuracy(testOriginal,testRandom)\n",
    "    accuMajority0,matMajority0=cMatrixAndAccuracy(testOriginal,testMajority0)\n",
    "    accuMajority4,matMajority4=cMatrixAndAccuracy(testOriginal,testMajority4)\n",
    "\n",
    "    print(\"Accuracy in case of random generation is : \",accuRandom)\n",
    "    print(\"Accuracy in case of Majority Predicton with 0 as majority is : \",accuMajority0)\n",
    "    print(\"Accuracy in case of Majority Predicton with 4 as majority is : \",accuMajority4)\n",
    "    \n",
    "differentAccuracies()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 1:d__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 359/359 [00:00<00:00, 7337.59it/s]\n",
      "100%|██████████| 1600000/1600000 [02:48<00:00, 9498.89it/s] \n"
     ]
    }
   ],
   "source": [
    "def removingStopWords(dataToStem):\n",
    "    #Here dataToStem is to be a panda DataFrame\n",
    "    \n",
    "    ps = PorterStemmer()\n",
    "    tweets=dataToStem[5]\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    finalSentenceList=[]\n",
    "    i=0\n",
    "    for sentences in tqdm(tweets):\n",
    "        temp=sentences.replace(\",\",\" \").replace(\".\",\" \").split()\n",
    "        \n",
    "        afterTwitterHandle=[word for word in temp if not word.startswith('@')]\n",
    "        \n",
    "        afterStopWord=[word for word in afterTwitterHandle if word not in stop_words]\n",
    "        \n",
    "        finalList=[ps.stem(word) for word in afterStopWord]\n",
    "        \n",
    "        sen=' '.join(finalList)\n",
    "        \n",
    "        finalSentenceList.append(sen)\n",
    "\n",
    "    justForTest=pd.DataFrame(finalSentenceList)\n",
    "    dataToStem[5]=justForTest\n",
    "    return dataToStem\n",
    "\n",
    "# print(test[5])\n",
    "cleanTestData=removingStopWords(test.copy())\n",
    "# print(cleanTestData[5])\n",
    "# print(\"checking if train is getting changed or not\")\n",
    "# print(train)\n",
    "cleanTrainData=removingStopWords(train.copy())\n",
    "# print(\"checking if train data is changed or not\")\n",
    "# print(train)\n",
    "# print(cleanTrainData)\n",
    "# print(cleanTestData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Tested On Cleaned Trained Data__\n",
    "Accuracy is  81.0645\n",
    "Confusion matrix is : \n",
    "\n",
    "        Class0  Class4\n",
    "Class0  670501  129499\n",
    "Class4  173469  626531\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Tested on Cleaned Tested Data__ \n",
    "printing 144 33 34 148\n",
    "Accuracy is  81.33704735376045\n",
    "Confusion matrix is : \n",
    "\n",
    "        Class0  Class4\n",
    "Class0     144      33\n",
    "Class4      34     148"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Tested on cleaned Test Data having Trained on original train data__\n",
    "Accuracy is  77.15877437325905\n",
    "Confusion matrix is : \n",
    "\n",
    "        Class0  Class4\n",
    "Class0     132      45\n",
    "Class4      37     145\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Tested on Original Test Data having trained on Original Trained Data__\n",
    "Accuracy is  80.77994428969359\n",
    "Confusion matrix is : \n",
    "\n",
    "        Class0  Class4\n",
    "Class0     142      35\n",
    "Class4      34     148\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Part 1(D) onwards__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF Part A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here one extra parameter min_df is added to increase the accuracy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "def tf_idf():\n",
    "    cleanedTrainTweet=list(cleanTrainData[5])\n",
    "    cleanedTestTweet=list(cleanTestData[5])\n",
    "    vectorizer = TfidfVectorizer() #min_df=0.0006 include it in parameter in vectorizer when need min-Df\n",
    "    trainFeatures=vectorizer.fit_transform(cleanedTrainTweet)\n",
    "    testFeatures=vectorizer.transform(cleanedTestTweet)\n",
    "    return trainFeatures,testFeatures\n",
    "trainFeatures,testFeatures=tf_idf() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trainFeatures.shape)\n",
    "print(\"----------------------------\")\n",
    "print(testFeatures.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Learning on training data model has been generated and stored so it will be just used later\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "def generateModel(trainAttrFeatures):\n",
    "    model = GaussianNB()\n",
    "    expectedOutput=cleanTrainData[0].to_numpy()\n",
    "    classes=np.unique(expectedOutput)\n",
    "    for i in tqdm(range(0,cleanTrainData[0].size,1000)):\n",
    "        model.partial_fit(trainAttrFeatures[i:i+1000].toarray(),expectedOutput[i:i+1000],classes)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model already there\n",
    "# with open('model','wb') as f:\n",
    "#     pickle.dump(model,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('model','rb') as f:\n",
    "#     model=pickle.load(f)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(attrModel,testAttrFeatures):\n",
    "    actualOutput=attrModel.predict(testAttrFeatures[:].toarray())\n",
    "    prob=attrModel.predict_log_proba(testAttrFeatures[:].toarray())[:,1]\n",
    "    accu=attrModel.score(testAttrFeatures[:].toarray(),list(cleanTestData[0]))\n",
    "    return accu,prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__For accuracy of model on all the features__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracyIncludingAll,probabilityIncludingAll=accuracy(model,testFeatures)\n",
    "print(accuracyIncludingAll)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ Accuracy for all the features is 49.58217270194986__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1600/1600 [00:31<00:00, 50.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken is  31.871193408966064\n",
      "Accuracy using minDf Model is  78.55153203342618  %\n"
     ]
    }
   ],
   "source": [
    "mindfModel=generateModel(trainFeatures)\n",
    "accuracyForMinDf,probabilityForMinDf=accuracy(mindfModel,testFeatures)\n",
    "print(\"Accuracy using minDf Model is \",accuracyForMinDf * 100,\" %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Accuracy using MinDf for tfidf without using select percentile value is 78.55153203342619__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF Part B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectPercentile, f_classif\n",
    "def tf_idfSelectPercentile():\n",
    "    trainFeat,testFeat=tf_idf()\n",
    "    support = SelectPercentile(f_classif, percentile=35)\n",
    "    support.fit(trainFeat,list(cleanTrainData[0]))\n",
    "    \n",
    "    trainPerFeatures = support.transform(trainFeat)\n",
    "    testPerFeatures = support.transform(testFeat)\n",
    "    \n",
    "    return trainPerFeatures,testPerFeatures\n",
    "st=time.time()\n",
    "trainPerFeatures,testPerFeatures=tf_idfSelectPercentile() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentileModel=generateModel(trainPerFeatures)\n",
    "print(\"time taken is \",time.time()-st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Storing Percentile Model\n",
    "# with open('percentileModel','wb') as f:\n",
    "#     pickle.dump(percentileModel,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accessing Percentile Model\n",
    "# with open('percentileModel','rb') as f:\n",
    "#     percentileModel=pickle.load(f)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(testPerFeatures.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7771587743732591\n"
     ]
    }
   ],
   "source": [
    "afterPercentileAccuracy,percentileProbability=accuracy(percentileModel,testPerFeatures)\n",
    "print(afterPercentileAccuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Accuracy after Select 10 Percentile is 55.71030640668524__ \n",
    "\n",
    "__Accuracy after Select 10 Percentile using MinDf as 0.0006 is 71.3091922005571__ \n",
    "\n",
    "__Accuracy after Select 30 Percentile is 49.303621169916434__\n",
    "\n",
    "__Accuracy after Select 5 Percentile is 57.93871866295265__\n",
    "\n",
    "__Accuracy after Select 5 Percentile using MinDf as 0.0006 is 64.62395543175488__\n",
    "\n",
    "__Accuracy after Select 2 Percentile is 69.63788300835655__\n",
    "\n",
    "__Accuracy after Select 2 Percentile using MinDf as 0.0006 is 60.16713091922006__\n",
    "\n",
    "__Accuracy after Select 35 Percentile using MinDf as 0.0006 is 77.71587743732591__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROC CURVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "from sklearn import metrics\n",
    "def roc(originOut,prob):\n",
    "    import matplotlib.pyplot as plt\n",
    "    falsePositive,truePositive,thresholds = metrics.roc_curve(originOut, prob, pos_label=4)\n",
    "    auc = metrics.roc_auc_score(originOut,prob)\n",
    "    plt.plot(falsePositive,truePositive,label='Area under curve is '+str(auc))\n",
    "    plt.xlabel(\"False Positive\")\n",
    "    plt.ylabel(\"True Positive\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 359/359 [00:00<00:00, 73883.96it/s]\n"
     ]
    }
   ],
   "source": [
    "# Roc curve for test having trained on Original Train Data\n",
    "# Always check for the parameter of main function, that's where training is being done.\n",
    "(original,tested,probability)=testData(test.copy(),\"uni\")\n",
    "roc(original,probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 359/359 [00:00<00:00, 81568.53it/s]\n"
     ]
    }
   ],
   "source": [
    "#Roc curve for cleaned test data having trained on Cleaned Test Data\n",
    "(original,tested,probability)=testData(cleanTestData.copy(),\"uni\")\n",
    "roc(original,probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Roc curve obtained after tfidf with 100 percentile, these all will be done on cleaned data only\n",
    "accuracyIncludingAll,probabilityIncludingAll=accuracy(model,testFeatures)\n",
    "originalValue=list(cleanTestData[0])\n",
    "roc(originalValue,probabilityIncludingAll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Roc curve obtained after tfidf with 10 percentile,for cleaned data only\n",
    "afterPercentileAccuracy,percentileProbability=accuracy(percentileModel,testPerFeatures)\n",
    "originalValue=list(cleanTestData[0])\n",
    "roc(originalValue,percentileProbability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is for TF-IDF using Min-Df\n",
    "mindfModel=generateModel(trainFeatures)\n",
    "accuracyForMinDf,probabilityForMinDf=accuracy(mindfModel,testFeatures)\n",
    "originalValue=list(cleanTestData[0])\n",
    "roc(originalValue,probabilityForMinDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is for select percentile using Min-Df\n",
    "mindfPercentileModel=generateModel(trainPerFeatures)\n",
    "accuForMinDf,probaForMinDf=accuracy(mindfPercentileModel,testPerFeatures)\n",
    "originalValue=list(cleanTestData[0])\n",
    "roc(originalValue,probaForMinDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 359/359 [00:00<00:00, 4164.28it/s]\n"
     ]
    }
   ],
   "source": [
    "#This is for Bigram\n",
    "(original,tested,probability)=testData(cleanTestData.copy(),\"bi\")\n",
    "roc(original,probability)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Part E: Feature Selection__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One feature bigram is already done earlier\n",
    "#Here nltk pos_tag will be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#It would be done on the cleaned Data\n",
    "import nltk \n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "taggedList=[]\n",
    "dataForTagging=cleanTrainData[5].to_numpy()\n",
    "\n",
    "for sentences in tqdm(dataForTagging):\n",
    "    wordList =  sentences.split()\n",
    "    taggedFinal = nltk.pos_tag(wordList)\n",
    "    taggedList.append(taggedFinal)\n",
    "#     break\n",
    "    \n",
    "# with open('taggedSentences','wb') as f:\n",
    "#     pickle.dump(taggedList,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('http://twitpic', 'NN'), ('com/2y1zl', 'SYM'), ('-', ':'), ('awww', 'NN'), (\"that'\", 'NN'), ('bummer', 'NN'), ('you', 'PRP'), ('shoulda', 'VBP'), ('got', 'VBD'), ('david', 'JJ'), ('carr', 'JJ'), ('third', 'JJ'), ('day', 'NN'), (';D', 'VBD')]\n"
     ]
    }
   ],
   "source": [
    "# with open('taggedSentences','rb') as f:\n",
    "#     finalTaggedList = pickle.load(f)\n",
    "\n",
    "print(finalTaggedList[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainNegative=cleanTrainData[cleanTrainData[0]==0][5]\n",
    "trainPositive=cleanTrainData[cleanTrainData[0]==4][5]\n",
    "\n",
    "positiveTaggedList=[]\n",
    "for sentences in tqdm(trainPositive):\n",
    "#     print(sentences)\n",
    "    wordList =  sentences.split()\n",
    "    taggedFinal = nltk.pos_tag(wordList)\n",
    "    positiveTaggedList.append(taggedFinal)\n",
    "    \n",
    "negativeTaggedList=[]\n",
    "for sentences in tqdm(trainNegative):\n",
    "#     print(sentences)\n",
    "    wordList =  sentences.split()\n",
    "    taggedFinal = nltk.pos_tag(wordList)\n",
    "    negativeTaggedList.append(taggedFinal)\n",
    "    \n",
    "# with open('positiveTagged','wb') as f:\n",
    "#     pickle.dump(positiveTaggedList,f)\n",
    "    \n",
    "# with open('negativeTagged','wb') as f:\n",
    "#     pickle.dump(negativeTaggedList,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('positiveTagged','rb') as f:\n",
    "#     positiveTaggedList = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('negativeTagged','rb') as f:\n",
    "#     negativeTaggedList = pickle.load(f)\n",
    "# # print(len(positiveTaggedList))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
